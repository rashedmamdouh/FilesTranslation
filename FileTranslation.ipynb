{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install fpdf\n",
        "!pip install deep_translator\n",
        "!pip install nltk\n",
        "!python -m nltk.downloader popular"
      ],
      "metadata": {
        "id": "rNC_k6i34yH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd40e3f5-2739-4023-939e-16598e92f875"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.10.4-py3-none-any.whl (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20221105 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.27.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (42.0.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20221105 pdfplumber-0.10.4 pypdfium2-4.27.0\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40703 sha256=d6db32a51637ff0c52c85193c8f3b6862791e0c38e1687ceafc81639f3303a26\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n",
            "Collecting deep_translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep_translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep_translator) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2024.2.2)\n",
            "Installing collected packages: deep_translator\n",
            "Successfully installed deep_translator-1.11.4\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from fpdf import FPDF\n",
        "import pdfplumber"
      ],
      "metadata": {
        "id": "Sd8Hw2nY5voq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extract the Text from the File**"
      ],
      "metadata": {
        "id": "NJhM5Z7AuDFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract(page):\n",
        "  \"\"\"Extract PDF text and Delete in-paragraph line breaks.\"\"\"\n",
        "  # Get text\n",
        "  extracted = page.extract_text()\n",
        "  # Delete in-paragraph line breaks\n",
        "  extracted = extracted.replace(\".\\n\", \"**/m\" # keep par breaks\n",
        "                      ).replace(\". \\n\", \"**/m\" # keep par breaks\n",
        "                      ).replace(\"\\n\", \"\" # delete in-par breaks\n",
        "                      ).replace(\"**/m\", \".\\n\\n\") # restore par break\n",
        "  return extracted"
      ],
      "metadata": {
        "id": "w4tuD5zwpTzF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Translation**"
      ],
      "metadata": {
        "id": "zXVWCg6OuUMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_extracted(Extracted):\n",
        "  \"\"\"Wrapper for Google Translate with upload workaround.\"\"\"\n",
        "  # Set-up and wrap translation client\n",
        "  translate = GoogleTranslator(source='auto', target='nl').translate\n",
        "  # Split input text into a list of sentences\n",
        "  sentences = sent_tokenize(Extracted)\n",
        "  # Initialize containers\n",
        "  translated_text = ''\n",
        "  source_text_chunk = ''\n",
        "  # collect chuncks of sentences, translate individually\n",
        "  for sentence in sentences:\n",
        "    # if chunck + current sentence < limit, add the sentence\n",
        "    if ((len(sentence.encode('utf-8')) +  len(source_text_chunk.encode('utf-8')) < 5000)):\n",
        "      source_text_chunk += ' ' + sentence\n",
        "    # else translate chunck and start new one with current sentence\n",
        "    else:\n",
        "      translated_text += ' ' + translate(source_text_chunk)\n",
        "     # if current sentence smaller than 5000 chars, start new chunck\n",
        "    if (len(sentence.encode('utf-8')) < 5000):\n",
        "       source_text_chunk = sentence\n",
        "     # else, replace sentence with notification message\n",
        "    else:\n",
        "       message = \"<<Omitted Word longer than 5000bytes>>\"\n",
        "       translated_text += ' ' + translate(message)\n",
        "       # Re-set text container to empty\n",
        "       source_text_chunk = ''\n",
        "  # Translate the final chunk of input text, if there is any valid   text left to translate\n",
        "  if translate(source_text_chunk) != None:\n",
        "    translated_text += ' ' + translate(source_text_chunk)\n",
        "  return translated_text"
      ],
      "metadata": {
        "id": "nHFu6E8Ohynd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open PDF\n",
        "with pdfplumber.open(\"/content/CV.pdf\") as pdf:\n",
        "  # Treat each page individually\n",
        "  for page in pdf.pages:\n",
        "    # Extract Page\n",
        "    extracted = extract(page)\n",
        "    # Translate Page\n",
        "    if extracted != \"\":\n",
        "      # Translate paragraphs individually to keep breaks\n",
        "      paragraphs = extracted.split(\"\\n\\n\")\n",
        "      translated = \"\\n\\n\".join(\n",
        "        [translate_extracted(paragraph) for paragraph in paragraphs]\n",
        "        )\n",
        "    else:\n",
        "      translated = extracted"
      ],
      "metadata": {
        "id": "eDOUBOlOkRw4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Output**"
      ],
      "metadata": {
        "id": "Iw0nZ8qVuZrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Write the extracted and translated text to PDF file\n",
        "fpdf = FPDF()\n",
        "fpdf.set_font(\"Helvetica\", size = 7)\n",
        "fpdf.add_page()\n",
        "fpdf.multi_cell(w=0, h=5,\n",
        "                txt= translated.encode(\"latin-1\",\n",
        "                                      errors = \"replace\"\n",
        "                              ).decode(\"latin-1\"))\n",
        "# Save all FPDF pages\n",
        "fpdf.output(\"/content/output.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T6NXQwMwu1rC",
        "outputId": "f4ddb389-bf9f-40bf-de10-41819dc1984a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the extracted and translated text to the output text file\n",
        "with open(\"/content/output.txt\", \"w\") as file:\n",
        "    file.write(translated)"
      ],
      "metadata": {
        "id": "07LRFpokrYVh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vFdbfcP6xJp3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}